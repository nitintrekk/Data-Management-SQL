{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift\n",
    "from kmodes.kprototypes import KPrototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\".//CSV'S/Using/reviews.csv\", error_bad_lines=False)\n",
    "reviews_bis = pd.read_csv(\".//CSV'S/Using/reviews_bis.csv\", error_bad_lines=False)\n",
    "\n",
    "# Join the dataframes based on the specified conditions\n",
    "reviews_merged = pd.merge(reviews, reviews_bis, on ='review_id', how='left')\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if not isinstance(text, str) or text == \"\":\n",
    "        return 'neutral'\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        return 'positive'\n",
    "    elif sentiment < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "reviews_merged['review_comment_message'] = reviews_merged['review_comment_message'].apply(preprocess_text)\n",
    "reviews_merged['sentiment'] = reviews_merged['review_comment_message'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from CSV files into pandas dataframes\n",
    "result = pd.read_excel(\".//CSV'S/Using/result_wip_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes based on the specified conditions\n",
    "result = pd.merge(result, reviews_merged[['order_id_x', 'sentiment']], left_on='order_id', right_on='order_id_x', how='left')\n",
    "\n",
    "# Drop the duplicate column\n",
    "result.drop('order_id_x', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values per column\n",
    "num_missing = result.isna().sum()\n",
    "\n",
    "# Display the number of missing values per column\n",
    "print(\"Number of missing values per column:\")\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill lead_type with the most frequent value\n",
    "most_frequent_lead_type = result['lead_type'].mode()[0]\n",
    "result['lead_type'] = result['lead_type'].fillna(most_frequent_lead_type)\n",
    "\n",
    "# replace NA values in the \"origin\" column with \"Other\"\n",
    "result['origin'].fillna('other', inplace=True)\n",
    "\n",
    "# replace \"Unknown\" values in the \"origin\" column with \"Other\"\n",
    "result['origin'].replace('unknown', 'other', inplace=True)\n",
    "\n",
    "# Fill lead_behaviour with the most frequent value\n",
    "most_frequent_lead_behavior = result['lead_behavior'].mode()[0]\n",
    "result['lead_behavior'] = result['lead_behavior'].fillna(most_frequent_lead_behavior)\n",
    "\n",
    "# Fill business_type with the most frequent value\n",
    "most_frequent_business_type = result['business_type'].mode()[0]\n",
    "result['business_type'] = result['business_type'].fillna(most_frequent_business_type)\n",
    "\n",
    "# Fill won_date_ie_date_of_first_saas_payment with median\n",
    "won_date_median = result['won_date_ie_date_of_first_saas_payment'].median()\n",
    "result['won_date_ie_date_of_first_saas_payment'] = result['won_date_ie_date_of_first_saas_payment'].fillna(won_date_median)\n",
    "\n",
    "# Fill order_date_time with median\n",
    "order_date_median = result['order_date_time'].median()\n",
    "result['order_date_time'] = result['order_date_time'].fillna(order_date_median)\n",
    "\n",
    "# Fill lead_type with the most frequent value\n",
    "most_frequent_sentiment_type = result['sentiment'].mode()[0]\n",
    "result['sentiment'] = result['sentiment'].fillna(most_frequent_sentiment_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ddaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values per column\n",
    "num_missing = result.isna().sum()\n",
    "\n",
    "# Display the number of missing values per column\n",
    "print(\"Number of missing values per column:\")\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e89d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframe to excel\n",
    "# result.to_excel('result_final_1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db563951",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = result.nunique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f845b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to datetime format\n",
    "result['registered_on_landing_page_date'] = pd.to_datetime(result['registered_on_landing_page_date'])\n",
    "result['won_date_ie_date_of_first_saas_payment'] = pd.to_datetime(result['won_date_ie_date_of_first_saas_payment'])\n",
    "\n",
    "# calculate the turnaround time in days\n",
    "result['turnaround_time'] = (result['won_date_ie_date_of_first_saas_payment'] - result['registered_on_landing_page_date']).dt.days\n",
    "\n",
    "# count the number of dates\n",
    "num_dates = result['turnaround_time'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to datetime format\n",
    "result['won_date_ie_date_of_first_saas_payment'] = pd.to_datetime(result['won_date_ie_date_of_first_saas_payment'])\n",
    "result['order_date_time'] = pd.to_datetime(result['order_date_time'])\n",
    "\n",
    "# calculate the turnaround time in days\n",
    "result['first_order_tat'] = (result['order_date_time'] - result['won_date_ie_date_of_first_saas_payment']).dt.days\n",
    "\n",
    "# count the number of dates\n",
    "num_dates = result['first_order_tat'].count()\n",
    "\n",
    "# print the number of dates\n",
    "print(f\"Number of dates: {num_dates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by business segment and calculate the average turnaround time\n",
    "avg_tat_by_segment = result.groupby('business_segment')['turnaround_time'].mean()\n",
    "\n",
    "# sort by ascending average turnaround time\n",
    "avg_tat_by_segment = avg_tat_by_segment.sort_values()\n",
    "\n",
    "# create a horizontal bar chart with adjusted spacing\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.barh(avg_tat_by_segment.index, avg_tat_by_segment.values, height=0.6)\n",
    "\n",
    "# add labels and title\n",
    "ax.set_ylabel('Business Segment')\n",
    "ax.set_xlabel('Average Turnaround Time')\n",
    "ax.set_title('Average Turnaround Time by Business Segment')\n",
    "\n",
    "# display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3921b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique values of sr_id, sdr_id, product_id, and merchant_id\n",
    "unique_sr_id = result['sr_id'].nunique()\n",
    "unique_sdr_id = result['sdr_id'].nunique()\n",
    "unique_product_id = result['product_id'].nunique()\n",
    "unique_merchant_id = result['merchant_id'].nunique()\n",
    "\n",
    "# print the counts\n",
    "print(\"Number of unique sr_id values:\", unique_sr_id)\n",
    "print(\"Number of unique sdr_id values:\", unique_sdr_id)\n",
    "print(\"Number of unique product_id values:\", unique_product_id)\n",
    "print(\"Number of unique merchant_id values:\", unique_merchant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd99887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique sr_id and sdr_id values\n",
    "unique_sr_id = result['sr_id'].unique()\n",
    "unique_sdr_id = result['sdr_id'].unique()\n",
    "\n",
    "# print the number of unique sr_id and sdr_id values\n",
    "print(\"Number of unique sr_id values:\", len(unique_sr_id))\n",
    "print(\"Number of unique sdr_id values:\", len(unique_sdr_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78375f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by merchant_id and count the number of unique product_id values for each group\n",
    "product_count_by_merchant_id = result.groupby('merchant_id')['product_id'].nunique().sort_values(ascending=False)\n",
    "\n",
    "# print the resulting Series\n",
    "print(product_count_by_merchant_id)\n",
    "\n",
    "# calculate the total number of unique products ordered by all merchants\n",
    "total_unique_products = product_count_by_merchant_id.sum()\n",
    "\n",
    "print(\"Total unique products ordered by all merchants:\", total_unique_products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f940d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming your DataFrame is called 'df' and has columns 'merchant_id' and 'product_id'\n",
    "unique_merchant_ids = result['merchant_id'].unique()\n",
    "total_product_count = 0\n",
    "for merchant_id in unique_merchant_ids:\n",
    "    product_count = result[result['merchant_id'] == merchant_id]['product_id'].count()\n",
    "    total_product_count += product_count\n",
    "    print(f\"Merchant ID: {merchant_id} - Product count: {product_count}\")\n",
    "print(f\"Total product count: {total_product_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed45e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_merchant_ids = result['merchant_id'].unique()\n",
    "unique_merchant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from a csv file\n",
    "df = result.copy()\n",
    "\n",
    "# group by business segment and count unique product ids\n",
    "product_count = df.groupby('business_segment')['product_id'].nunique()\n",
    "\n",
    "# sort values in descending order\n",
    "product_count = product_count.sort_values(ascending=False)\n",
    "\n",
    "# create a horizontal bar chart\n",
    "product_count.plot(kind='barh')\n",
    "plt.xlabel('Number of Products')\n",
    "plt.ylabel('Business Segment')\n",
    "plt.title('Number of Products Sold in Each Business Segment')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45749f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from a csv file\n",
    "df = result.copy()\n",
    "\n",
    "# extract the month from the order date\n",
    "df['month'] = df['order_date_time'].dt.month_name()\n",
    "\n",
    "# define the order of the months\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# convert the month column to a categorical data type with the specified order\n",
    "df['month'] = pd.Categorical(df['month'], categories=month_order, ordered=True)\n",
    "\n",
    "# group by business segment, month, and product id and count unique product ids\n",
    "product_count = df.groupby(['business_segment', 'month'])['product_id'].nunique().reset_index()\n",
    "\n",
    "# create a pivot table to reshape the data\n",
    "product_count_pivot = product_count.pivot(index='business_segment', columns='month', values='product_id')\n",
    "\n",
    "# increase the size of the heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# create a heatmap to visualize the data\n",
    "im = ax.imshow(product_count_pivot, cmap='Blues')\n",
    "\n",
    "# set x-tick labels and rotation\n",
    "ax.set_xticks(range(len(product_count_pivot.columns)))\n",
    "ax.set_xticklabels(product_count_pivot.columns, rotation=45, ha='right')\n",
    "\n",
    "# set y-tick labels\n",
    "ax.set_yticks(range(len(product_count_pivot.index)))\n",
    "ax.set_yticklabels(product_count_pivot.index)\n",
    "\n",
    "# add x and y axis labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Business Segment')\n",
    "plt.title('Number of Products Sold by Segment and Month')\n",
    "\n",
    "# add colorbar\n",
    "plt.colorbar(im)\n",
    "\n",
    "# display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dummies = result[['lead_id', 'registered_on_landing_page_date', 'lead_type', 'origin', 'business_segment', 'lead_behavior', 'has_company', 'has_gtin', 'business_type', 'won_date_ie_date_of_first_saas_payment', 'order_line_item', 'order_date_time', 'price', 'freight_cost', 'turnaround_time', 'first_order_tat','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns for which you want to compute the correlation matrix\n",
    "cols = ['registered_on_landing_page_date', 'has_company', 'has_gtin', 'price', 'freight_cost', 'turnaround_time', 'first_order_tat','sentiment']\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = result_dummies[cols].corr()\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff371b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dummy variables for the columns\n",
    "dummy_cols = ['lead_type', 'origin', 'business_segment', 'lead_behavior', 'business_type','sentiment']\n",
    "result_dummies = pd.get_dummies(result_dummies, columns=dummy_cols)\n",
    "\n",
    "# Map True and False values to 1 and 0\n",
    "result_dummies['has_company'] = result_dummies['has_company'].map({True: 1, False: 0})\n",
    "result_dummies['has_gtin'] = result_dummies['has_gtin'].map({True: 1, False: 0})\n",
    "\n",
    "# Extract month and year from 'registered_on_landing_page_date'\n",
    "result_dummies['registered_on_landing_page_month'] = pd.to_datetime(result_dummies['registered_on_landing_page_date']).dt.month\n",
    "result_dummies['registered_on_landing_page_year'] = pd.to_datetime(result_dummies['registered_on_landing_page_date']).dt.year\n",
    "\n",
    "# Extract month and year from 'won_date_ie_date_of_first_saas_payment'\n",
    "result_dummies['won_date_month'] = pd.to_datetime(result_dummies['won_date_ie_date_of_first_saas_payment']).dt.month\n",
    "result_dummies['won_date_year'] = pd.to_datetime(result_dummies['won_date_ie_date_of_first_saas_payment']).dt.year\n",
    "\n",
    "# Extract month and year from 'order_date_time'\n",
    "result_dummies['order_date_month'] = pd.to_datetime(result_dummies['order_date_time']).dt.month\n",
    "result_dummies['order_date_year'] = pd.to_datetime(result_dummies['order_date_time']).dt.year\n",
    "\n",
    "# Drop original columns\n",
    "result_dummies = result_dummies.drop(['registered_on_landing_page_date', 'won_date_ie_date_of_first_saas_payment', 'order_date_time','lead_id'], axis=1)\n",
    "\n",
    "result_dummies.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_data, test_data = train_test_split(result_dummies, test_size=0.30, random_state=100)\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters = 3\n",
    "\n",
    "# Instantiate the models\n",
    "models = [\n",
    "    KMeans(n_clusters=n_clusters, n_init=10, random_state=42),\n",
    "    AgglomerativeClustering(n_clusters=n_clusters),\n",
    "    DBSCAN(),\n",
    "    KPrototypes(n_clusters=n_clusters, init='Cao', verbose=0)\n",
    "]\n",
    "\n",
    "# Train and evaluate the models\n",
    "for model in models:\n",
    "    if isinstance(model, (AgglomerativeClustering, DBSCAN)):\n",
    "        test_labels = model.fit_predict(test_data)\n",
    "    elif isinstance(model, KPrototypes):\n",
    "        model.fit(train_data, categorical=list(range(6)))\n",
    "        test_labels = model.predict(test_data, categorical=list(range(6)))\n",
    "        print(f\"{type(model).__name__} Predicted Labels: {test_labels}\")\n",
    "    else:\n",
    "        model.fit(train_data)\n",
    "        test_labels = model.predict(test_data)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(test_data, test_labels)\n",
    "    \n",
    "    # Print the silhouette score for the model\n",
    "    print(f\"{type(model).__name__} Silhouette Score: {silhouette_avg:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
